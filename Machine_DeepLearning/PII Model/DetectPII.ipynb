{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee7e569-2b8a-4262-bb2f-d9e087f97a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('../input/pii-detection-removal-from-educational-data/train.json', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad53b1b-1298-41df-b182-c024c7b6452e",
   "metadata": {},
   "source": [
    "### Data De-identification—Key Concepts and Strategies\n",
    "\n",
    "\"Privacy of individual student records is protected under FERPA (Family Educational Rights and Privacy Act ). To avoid unauthorized disclosure of personally identifiable information from education records (PII), students’ data must be adequately protected at all times. For example, when schools, districts, or states publish reports on student achievement or share students’ data with external researchers, these organizations should apply disclosure avoidance strategies, to prevent unauthorized release of information about individual students. To ensure successful data protection, it is essential that techniques are appropriate for the intended purpose and that their application follows the best practices.\"\r\n",
    "\r\n",
    "ANONYMIZATION\r\n",
    "\r\n",
    "\"Anonymization of data refers to the process of data de-identification which produces de-identified data, where individual records cannot be linked back to an original student record system or to other individual records from the same source, because they do not include a record code needed to link the records. As such, anonymized data are not useful for monitoring the progress and performance of individual students; however, they can be used for other research or training purposes. An anonymized data file could be produced from the de-identified file that contains record codes by removing the codes and reviewing the resulting file to ensure that additional disclosure limitation methods do not need to be applied. The documentation for the anonymized data file should identify any disclosure limitation techniques that were applied and their implications for the analysis.\"\r\n",
    "\r\n",
    "BLURRING\r\n",
    "\r\n",
    "\"Blurring is a disclosure limitation method which is used to reduce the precision of the disclosed data to minimize the certainty of individual identification. There are many possible ways to implement blurring, such as by converting continuous data elements into categorical data elements (e.g., creating categories that subsume unique cases), aggregating data across small groups of respondents, and reporting rounded values and ranges instead of exact counts to reduce the certainty of identification. Another approach involves replacing an individual’s actual reported value with the average group value; it may be performed on more than one variable with different groupings for each variable.\"\r\n",
    "\r\n",
    "DE-IDENTIFICATION\r\n",
    "\r\n",
    "\"De-identification of data refers to the process of removing or obscuring any personally identifiable information from student records in a way that minimizes the risk of unintended disclosure of the identity of individuals and information about them. Specific steps and methods used to de-identify information (see disclosure limitation method for details) may vary depending on the circumstances, but should be appropriate to protect the confidentiality of the individuals. While it may not be possible to remove the disclosure risk completely, de-identification is considered successful when there is no reasonable basis to believe that the remaining information in the records can be used to identify an individual.\"\r\n",
    "\r\n",
    "MASKING\r\n",
    "\r\n",
    "\"Masking is a disclosure limitation method that is used to “mask” the original values in a data set to achieve data privacy protection. This general approach uses various techniques, such as data perturbation, to replace sensitive information with realistic but inauthentic data or modifies original data values based on pre-determined masking rules (e.g., by applying a transformation algorithm). The purpose of this technique is to retain the structure and functional usability of the data, while concealing information that could lead to the identification, either directly or indirectly, of an individual student.\"\r\n",
    "\r\n",
    "PERTURBATION\r\n",
    "\r\n",
    "\"Perturbation is a disclosure limitation method which involves making small changes to the data to prevent identification of individuals from unique or rare population groups. Data perturbation is a data masking technique in that it is used to “mask” the original values in a data set to avoid disclosure.\"\r\n",
    "\r\n",
    "REDACTION\r\n",
    "\r\n",
    "\"Redaction is a general term describing the process of expunging sensitive data from the records prior to disclosure in a way that meets established disclosure requirements applicable to the specific data disclosure occurrence (e.g., removing or obscuring PII from published reports to meet federal, state, and local privacy laws as well as organizational data disclosure policies).\"\r\n",
    "\r\n",
    "https://studentprivacy.ed.gov/sites/default/files/resource_document/file/data_deidentification_terms_0.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4f436-48d3-4e23-ba6a-aba84e21e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('new_file.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c24f2-4c59-40f9-8920-230ca712a145",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "Where can we find PII (Personal Identifiable Information) in our data?\r\n",
    "\r\n",
    "\"Data repositories filled with customer data, biometric data, third-party data, and user-generated content (UGC) serve as the lifeblood for enterprise AI solutions, but they are fraught with the risk of PII exposure. Customer data is a significant reservoir of PII.\"\r\n",
    "\r\n",
    "How to detect PII in your data?\r\n",
    "\r\n",
    "\"Large Language Models (LLMs) and foundation models emerge as powerful tools that transcend these constraints, paving the way for enhanced PII detection and extraction. These models, trained on extensive datasets, can untangle intricate language patterns, understand context, and handle ambiguities, thereby offering a more nuanced approach to PII detection.\"\r\n",
    "\r\n",
    "A few ways in which LLMs outperform regex in this\n",
    "* Contextual Understanding\n",
    "* Learning from data\n",
    "* Adaptability\n",
    "* Efficiency\n",
    "* Less maintenanceness maintenance\r\n",
    "\r\n",
    "https://labelbox.com/blog/how-to-detect-and-extract-personal-information-from-datasets-for-ai/\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e46f1-7aed-47eb-82df-bdfd5d053ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d614bc-0671-45b8-878e-54996675a24f",
   "metadata": {},
   "source": [
    "### Data Anoymization Using Faker\n",
    "\n",
    "All by Carl:\r\n",
    "\r\n",
    "Data anonymization using Faker (Titanic example) - By Carl McBride Ellis\r\n",
    "\r\n",
    "LINKS:\r\n",
    "\r\n",
    "    Faker (GitHub)\r\n",
    "    Faker documentation\r\n",
    "\r\n",
    "RELATED READING:\r\n",
    "\r\n",
    "    Data anonymization\r\n",
    "    Pseudonymization\r\n",
    "    Data re-identification\r\n",
    "    General Data Protection Regulation (GDPR) (EU) 2016/679\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
